{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jw-qCcdIS8q4"
      },
      "source": [
        "# Что сделал:\n",
        "\n",
        "- добавил выход из jarvis_loop при final_answer + защита от зацикливания\n",
        "- дополнил docs разной инфой классик мл\n",
        "- добавил RAG(простенький класс для загрузки доков, сохранения;  использование при ответе по флагу) -> точность стабильно выросла с 0.9 до 1.0\n",
        "- добавил timeout в ColabExecutor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6N_1LrIQjL-7"
      },
      "source": [
        "- Analyst Agent (решает: нужна команда)\n",
        "- Command Agent (генерирует код)\n",
        "- Colab Runtime (исполняет)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfiTSKT4hFin"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/riter/anaconda3/envs/autorag/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import uuid\n",
        "import json\n",
        "import faiss\n",
        "import os\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "client = OpenAI(\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        "    api_key=os.environ[\"OPENROUTER_API_KEY\"],\n",
        ")\n",
        "\n",
        "def llm_chat(system, user):\n",
        "    completion = client.chat.completions.create(\n",
        "        model=\"openai/gpt-5-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system},\n",
        "            {\"role\": \"user\", \"content\": user},\n",
        "        ],\n",
        "    )\n",
        "    return completion.choices[0].message.content.strip()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vrKYDqSJGRb8"
      },
      "outputs": [],
      "source": [
        "class MemoryAgent:\n",
        "    def __init__(self, dim=384):\n",
        "        self.model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "        self.index = faiss.IndexFlatL2(dim)\n",
        "        self.store = {}\n",
        "\n",
        "    def add(self, text: str):\n",
        "        emb = self.model.encode([text])\n",
        "        self.index.add(emb)\n",
        "        self.store[self.index.ntotal - 1] = text\n",
        "\n",
        "    def search(self, query: str, k=3):\n",
        "        if self.index.ntotal == 0:\n",
        "            return []\n",
        "        q_emb = self.model.encode([query])\n",
        "        _, idx = self.index.search(q_emb, k)\n",
        "        return [self.store[i] for i in idx[0] if i in self.store]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DocumentChunker:\n",
        "    \"\"\"Разбивает длинные документы на чанки с перекрытием.\"\"\"\n",
        "    \n",
        "    def __init__(self, chunk_size=500, overlap=50):\n",
        "        self.chunk_size = chunk_size\n",
        "        self.overlap = overlap\n",
        "    \n",
        "    def chunk(self, text: str, source: str = None) -> list:\n",
        "        \"\"\"Разбивает текст на чанки с метаданными.\n",
        "        \n",
        "        Args:\n",
        "            text: Исходный текст для разбиения\n",
        "            source: Источник документа (путь к файлу, URL и т.д.)\n",
        "        \n",
        "        Returns:\n",
        "            Список словарей с ключами: text, source, chunk_id\n",
        "        \"\"\"\n",
        "        if len(text) <= self.chunk_size:\n",
        "            return [{\"text\": text, \"source\": source, \"chunk_id\": 0}]\n",
        "        \n",
        "        chunks = []\n",
        "        start = 0\n",
        "        chunk_id = 0\n",
        "        \n",
        "        while start < len(text):\n",
        "            end = start + self.chunk_size\n",
        "            chunk_text = text[start:end]\n",
        "            \n",
        "            # Пытаемся разбить по границе предложения или слова\n",
        "            if end < len(text):\n",
        "                # Ищем последнюю точку или пробел\n",
        "                last_period = chunk_text.rfind('. ')\n",
        "                last_space = chunk_text.rfind(' ')\n",
        "                \n",
        "                if last_period > self.chunk_size // 2:\n",
        "                    end = start + last_period + 1\n",
        "                    chunk_text = text[start:end]\n",
        "                elif last_space > self.chunk_size // 2:\n",
        "                    end = start + last_space\n",
        "                    chunk_text = text[start:end]\n",
        "            \n",
        "            chunks.append({\n",
        "                \"text\": chunk_text.strip(),\n",
        "                \"source\": source,\n",
        "                \"chunk_id\": chunk_id\n",
        "            })\n",
        "            \n",
        "            start = end - self.overlap if end - self.overlap > start else end\n",
        "            chunk_id += 1\n",
        "        \n",
        "        return chunks\n",
        "    \n",
        "    def chunk_documents(self, documents: list, source_key: str = None) -> list:\n",
        "        \"\"\"Разбивает список документов на чанки.\n",
        "        \n",
        "        Args:\n",
        "            documents: Список строк или словарей с текстом\n",
        "            source_key: Ключ для получения источника из словаря\n",
        "        \n",
        "        Returns:\n",
        "            Список чанков\n",
        "        \"\"\"\n",
        "        all_chunks = []\n",
        "        for i, doc in enumerate(documents):\n",
        "            if isinstance(doc, dict):\n",
        "                text = doc.get(\"text\", str(doc))\n",
        "                source = doc.get(source_key) if source_key else f\"doc_{i}\"\n",
        "            else:\n",
        "                text = str(doc)\n",
        "                source = f\"doc_{i}\"\n",
        "            \n",
        "            chunks = self.chunk(text, source)\n",
        "            all_chunks.extend(chunks)\n",
        "        \n",
        "        return all_chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Ok4nRJmZGTNi"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "class RetrieverAgent:\n",
        "    def __init__(self, docs=None, dim=384):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            docs: Опциональный список документов для инициализации.\n",
        "                  Может быть списком строк или словарей с ключом 'text'.\n",
        "            dim: Размерность эмбеддингов\n",
        "        \"\"\"\n",
        "        self.model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "        self.index = faiss.IndexFlatL2(dim)\n",
        "        self.documents = []\n",
        "        self.dim = dim\n",
        "        \n",
        "        if docs:\n",
        "            self.add_documents(docs)\n",
        "    \n",
        "    def add_documents(self, docs: list):\n",
        "        \"\"\"Добавляет документы в индекс.\n",
        "        \n",
        "        Args:\n",
        "            docs: Список строк или словарей с ключом 'text'\n",
        "        \"\"\"\n",
        "        # Нормализуем формат документов\n",
        "        normalized_docs = []\n",
        "        for i, doc in enumerate(docs):\n",
        "            if isinstance(doc, dict):\n",
        "                normalized_docs.append({\n",
        "                    \"text\": doc.get(\"text\", str(doc)),\n",
        "                    \"source\": doc.get(\"source\", f\"doc_{len(self.documents) + i}\"),\n",
        "                    \"metadata\": doc.get(\"metadata\", {})\n",
        "                })\n",
        "            else:\n",
        "                normalized_docs.append({\n",
        "                    \"text\": str(doc),\n",
        "                    \"source\": f\"doc_{len(self.documents) + i}\",\n",
        "                    \"metadata\": {}\n",
        "                })\n",
        "        \n",
        "        # Создаём эмбеддинги\n",
        "        texts = [d[\"text\"] for d in normalized_docs]\n",
        "        embeddings = self.model.encode(texts)\n",
        "        \n",
        "        # Добавляем в индекс\n",
        "        self.index.add(np.array(embeddings).astype('float32'))\n",
        "        self.documents.extend(normalized_docs)\n",
        "        \n",
        "        print(f\"Добавлено {len(normalized_docs)} документов. Всего: {len(self.documents)}\")\n",
        "    \n",
        "    def add_from_file(self, filepath: str, chunker: DocumentChunker = None):\n",
        "        \"\"\"Загружает документы из файла.\n",
        "        \n",
        "        Args:\n",
        "            filepath: Путь к текстовому файлу\n",
        "            chunker: Опциональный DocumentChunker для разбиения на чанки\n",
        "        \"\"\"\n",
        "        with open(filepath, 'r', encoding='utf-8') as f:\n",
        "            text = f.read()\n",
        "        \n",
        "        if chunker:\n",
        "            chunks = chunker.chunk(text, source=filepath)\n",
        "        else:\n",
        "            chunks = [{\"text\": text, \"source\": filepath}]\n",
        "        \n",
        "        self.add_documents(chunks)\n",
        "    \n",
        "    def search(self, query: str, k: int = 3) -> list:\n",
        "        \"\"\"Поиск релевантных документов.\n",
        "        \n",
        "        Args:\n",
        "            query: Поисковый запрос\n",
        "            k: Количество результатов\n",
        "        \n",
        "        Returns:\n",
        "            Список словарей с ключами: text, source, metadata, score\n",
        "        \"\"\"\n",
        "        if self.index.ntotal == 0:\n",
        "            return []\n",
        "        \n",
        "        # Ограничиваем k количеством документов\n",
        "        k = min(k, self.index.ntotal)\n",
        "        \n",
        "        q_emb = self.model.encode([query])\n",
        "        distances, indices = self.index.search(np.array(q_emb).astype('float32'), k)\n",
        "        \n",
        "        results = []\n",
        "        for i, idx in enumerate(indices[0]):\n",
        "            if idx < len(self.documents) and idx >= 0:\n",
        "                doc = self.documents[idx].copy()\n",
        "                doc[\"score\"] = float(distances[0][i])\n",
        "                results.append(doc)\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def search_texts(self, query: str, k: int = 3) -> list:\n",
        "        \"\"\"Поиск релевантных документов, возвращает только тексты.\n",
        "        \n",
        "        Args:\n",
        "            query: Поисковый запрос\n",
        "            k: Количество результатов\n",
        "        \n",
        "        Returns:\n",
        "            Список строк с текстами документов\n",
        "        \"\"\"\n",
        "        results = self.search(query, k)\n",
        "        return [r[\"text\"] for r in results]\n",
        "    \n",
        "    def save(self, path: str):\n",
        "        \"\"\"Сохраняет индекс и документы на диск.\n",
        "        \n",
        "        Args:\n",
        "            path: Базовый путь (без расширения)\n",
        "        \"\"\"\n",
        "        faiss.write_index(self.index, f\"{path}.index\")\n",
        "        with open(f\"{path}.docs\", \"wb\") as f:\n",
        "            pickle.dump(self.documents, f)\n",
        "        print(f\"Сохранено в {path}.index и {path}.docs\")\n",
        "    \n",
        "    def load(self, path: str):\n",
        "        \"\"\"Загружает индекс и документы с диска.\n",
        "        \n",
        "        Args:\n",
        "            path: Базовый путь (без расширения)\n",
        "        \"\"\"\n",
        "        self.index = faiss.read_index(f\"{path}.index\")\n",
        "        with open(f\"{path}.docs\", \"rb\") as f:\n",
        "            self.documents = pickle.load(f)\n",
        "        print(f\"Загружено {len(self.documents)} документов из {path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "yIFrR-SdGTaS"
      },
      "outputs": [],
      "source": [
        "import signal\n",
        "\n",
        "class ColabExecutor:\n",
        "    def __init__(self, timeout: int = 30):\n",
        "        self.timeout = timeout\n",
        "    \n",
        "    def _timeout_handler(self, signum, frame):\n",
        "        raise TimeoutError(f\"Execution timed out after {self.timeout} seconds\")\n",
        "    \n",
        "    def run(self, code: str):\n",
        "        # Устанавливаем таймаут\n",
        "        old_handler = signal.signal(signal.SIGALRM, self._timeout_handler)\n",
        "        signal.alarm(self.timeout)\n",
        "        \n",
        "        try:\n",
        "            local_env = {}\n",
        "            exec(code, {}, local_env)\n",
        "            return {\n",
        "                \"status\": \"success\",\n",
        "                \"output\": str(local_env)\n",
        "            }\n",
        "        except TimeoutError as e:\n",
        "            return {\n",
        "                \"status\": \"error\",\n",
        "                \"traceback\": str(e)\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"status\": \"error\",\n",
        "                \"traceback\": str(e)\n",
        "            }\n",
        "        finally:\n",
        "            signal.alarm(0)\n",
        "            signal.signal(signal.SIGALRM, old_handler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3WeSPjmzGTm3"
      },
      "outputs": [],
      "source": [
        "def command_agent(task: str, memory: MemoryAgent, executor: ColabExecutor):\n",
        "    # 1. генерация кода\n",
        "    code = client.chat.completions.create(\n",
        "        model=\"openai/gpt-5-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"Сгенерируй Python-код. Только код.\"},\n",
        "            {\"role\": \"user\", \"content\": task},\n",
        "        ],\n",
        "    ).choices[0].message.content\n",
        "\n",
        "    # 2. выполнение\n",
        "    result = executor.run(code)\n",
        "\n",
        "    # 3. автодебаг\n",
        "    retries = 2\n",
        "    while result[\"status\"] == \"error\" and retries > 0:\n",
        "        code = client.chat.completions.create(\n",
        "            model=\"openai/gpt-5-mini\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"Исправь ошибку и верни только код.\"},\n",
        "                {\"role\": \"user\", \"content\": f\"Код:\\n{code}\\nОшибка:\\n{result['traceback']}\"}\n",
        "            ]\n",
        "        ).choices[0].message.content\n",
        "\n",
        "        result = executor.run(code)\n",
        "        retries -= 1\n",
        "\n",
        "    # 4. ревью\n",
        "    review = client.chat.completions.create(\n",
        "        model=\"openai/gpt-5-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"Сделай краткое техническое ревью.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Код:\\n{code}\\nРезультат:\\n{result}\"}\n",
        "        ]\n",
        "    ).choices[0].message.content\n",
        "\n",
        "    memory.add(f\"Task: {task}\\nCode:\\n{code}\\nResult:\\n{result}\")\n",
        "\n",
        "    return f\"{result}\\n\\nREVIEW:\\n{review}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NJgTm348GTzO"
      },
      "outputs": [],
      "source": [
        "functions = [\n",
        "    {\n",
        "        \"name\": \"search_docs\",\n",
        "        \"description\": \"Поиск по документации\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"query\": {\"type\": \"string\"}\n",
        "            },\n",
        "            \"required\": [\"query\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"run_code\",\n",
        "        \"description\": \"Сгенерировать и выполнить код\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"task\": {\"type\": \"string\"}\n",
        "            },\n",
        "            \"required\": [\"task\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"final_answer\",\n",
        "        \"description\": \"Финальный ответ пользователю\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"answer\": {\"type\": \"string\"}\n",
        "            },\n",
        "            \"required\": [\"answer\"]\n",
        "        }\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "z0LjguTTHQ7y"
      },
      "outputs": [],
      "source": [
        "def dispatch(name, args, retriever, memory, executor):\n",
        "    if name == \"search_docs\":\n",
        "        query = args.get(\"query\") or args.get(\"q\") or str(args)\n",
        "        results = retriever.search(query, k=5)\n",
        "        # Форматируем результаты для LLM\n",
        "        formatted = []\n",
        "        for r in results:\n",
        "            text = r.get(\"text\", str(r))\n",
        "            source = r.get(\"source\", \"unknown\")\n",
        "            formatted.append(f\"[{source}] {text}\")\n",
        "        return \"\\n\".join(formatted) if formatted else \"Документы не найдены\"\n",
        "\n",
        "    if name == \"run_code\":\n",
        "        task = args.get(\"task\") or args.get(\"code\") or args.get(\"description\") or str(args)\n",
        "        return command_agent(task, memory, executor)\n",
        "\n",
        "    if name == \"final_answer\":\n",
        "        answer = args.get(\"answer\") or args.get(\"text\") or args.get(\"response\") or args.get(\"message\") or str(args)\n",
        "        return answer\n",
        "\n",
        "    raise ValueError(f\"Unknown action: {name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "pZ4oOweHHUhY"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def jarvis_loop(user_input, retriever, memory, executor, use_rag_context=True, verbose=False):\n",
        "    # Получаем релевантную документацию из RAG\n",
        "    if use_rag_context:\n",
        "        relevant_docs = retriever.search(user_input, k=5)\n",
        "        doc_context = \"\\n\".join([f\"- {d['text']}\" for d in relevant_docs])\n",
        "    else:\n",
        "        doc_context = \"\"\n",
        "    \n",
        "    # Получаем релевантную историю из памяти\n",
        "    history = memory.search(user_input, k=2)\n",
        "    history_context = \"\\n\".join(history) if history else \"\"\n",
        "    \n",
        "    # Формируем системный промпт с контекстом\n",
        "    system_content = \"Ты JARVIS — AI ассистент программиста.\\n\\n\"\n",
        "    \n",
        "    if doc_context:\n",
        "        system_content += f\"Релевантная документация:\\n{doc_context}\\n\\n\"\n",
        "    \n",
        "    if history_context:\n",
        "        system_content += f\"История взаимодействий:\\n{history_context}\\n\\n\"\n",
        "    \n",
        "    system_content += (\n",
        "        \"Используй документацию для генерации правильного кода.\\n\"\n",
        "        \"Если нужно выполнить задачу, ответь строго в JSON формате:\\n\"\n",
        "        '{\"action\": \"run_code\", \"args\": {\"task\": \"описание задачи\"}}\\n'\n",
        "        '{\"action\": \"search_docs\", \"args\": {\"query\": \"поисковый запрос\"}}\\n'\n",
        "        '{\"action\": \"final_answer\", \"args\": {\"answer\": \"финальный ответ\"}}'\n",
        "    )\n",
        "    \n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_content},\n",
        "        {\"role\": \"user\", \"content\": user_input}\n",
        "    ]\n",
        "\n",
        "    iterations = 0\n",
        "    while iterations < 10:\n",
        "        iterations += 1\n",
        "        \n",
        "        response = client.chat.completions.create(\n",
        "            model=\"openai/gpt-5-mini\",\n",
        "            messages=messages\n",
        "        )\n",
        "\n",
        "        msg_text = response.choices[0].message.content.strip()\n",
        "\n",
        "        # Пытаемся разобрать JSON\n",
        "        try:\n",
        "            msg_json = json.loads(msg_text)\n",
        "            action = msg_json.get(\"action\")\n",
        "            args = msg_json.get(\"args\", {})\n",
        "        except Exception:\n",
        "            # Если LLM не вернул JSON, выдаём как финальный ответ\n",
        "            memory.add(f\"USER: {user_input}\\nASSISTANT: {msg_text}\")\n",
        "            return msg_text\n",
        "\n",
        "        # Диспетчер\n",
        "        result = dispatch(action, args, retriever, memory, executor)\n",
        "\n",
        "        # добавляем результат в контекст для следующего шага\n",
        "        messages.append({\"role\": \"assistant\", \"content\": msg_text})\n",
        "        messages.append({\"role\": \"user\", \"content\": f\"Результат {action}: {result}\"})\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Action: {action}\")\n",
        "            print(f\"Result: {result[:200]}...\" if len(str(result)) > 200 else f\"Result: {result}\")\n",
        "            print()\n",
        "\n",
        "        if action == \"final_answer\":\n",
        "            memory.add(f\"USER: {user_input}\\nASSISTANT: {result}\")\n",
        "            return result\n",
        "    max_iterations_msg = \"Превышено максимальное количество итераций. Попробуйте переформулировать запрос.\"\n",
        "    return max_iterations_msg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "7pgbcu6NHuyV"
      },
      "outputs": [],
      "source": [
        "docs = [\n",
        "    # sklearn - классификация\n",
        "    \"RandomForestClassifier — ансамблевый метод на основе деревьев решений. Параметры: n_estimators (количество деревьев), max_depth (глубина), random_state. Пример: RandomForestClassifier(n_estimators=100, random_state=42)\",\n",
        "    \"LogisticRegression — линейная модель для бинарной и мультиклассовой классификации. Параметры: C (регуляризация), solver, max_iter.\",\n",
        "    \"SVC (Support Vector Classifier) — классификатор на основе опорных векторов. Параметры: kernel (linear, rbf, poly), C, gamma.\",\n",
        "    \"GradientBoostingClassifier — градиентный бустинг для классификации. Параметры: n_estimators, learning_rate, max_depth.\",\n",
        "    \"XGBClassifier — оптимизированный градиентный бустинг из библиотеки xgboost. Быстрее sklearn GradientBoosting. Пример: XGBClassifier(n_estimators=100, learning_rate=0.1)\",\n",
        "    \"DecisionTreeClassifier — дерево решений для классификации. Параметры: max_depth, min_samples_split, criterion (gini/entropy).\",\n",
        "    \"KNeighborsClassifier — метод k ближайших соседей. Параметры: n_neighbors, weights (uniform/distance), metric.\",\n",
        "    \n",
        "    # sklearn - регрессия\n",
        "    \"LinearRegression — линейная регрессия методом наименьших квадратов. fit(X, y), predict(X), coef_, intercept_.\",\n",
        "    \"Ridge и Lasso — регрессия с L2 и L1 регуляризацией соответственно. Параметр alpha контролирует силу регуляризации.\",\n",
        "    \"RandomForestRegressor — ансамблевая регрессия на деревьях. Аналогичен классификатору, но для непрерывных целевых переменных.\",\n",
        "    \"GradientBoostingRegressor — градиентный бустинг для регрессии. Параметры: n_estimators, learning_rate, max_depth.\",\n",
        "    \n",
        "    # sklearn - метрики\n",
        "    \"accuracy_score — доля правильных предсказаний. from sklearn.metrics import accuracy_score; accuracy_score(y_true, y_pred)\",\n",
        "    \"precision_score, recall_score, f1_score — метрики для классификации. Параметр average='macro'/'micro'/'weighted' для мультикласса.\",\n",
        "    \"confusion_matrix — матрица ошибок классификации. Строки — истинные классы, столбцы — предсказанные.\",\n",
        "    \"mean_squared_error, mean_absolute_error — метрики регрессии. MSE штрафует большие ошибки сильнее.\",\n",
        "    \"r2_score — коэффициент детерминации R^2. Показывает долю объяснённой дисперсии. 1.0 — идеальная модель.\",\n",
        "    \"classification_report — сводка precision, recall, f1 по классам. from sklearn.metrics import classification_report\",\n",
        "    \"roc_auc_score — площадь под ROC-кривой. Метрика качества бинарной классификации.\",\n",
        "    \n",
        "    # sklearn - препроцессинг\n",
        "    \"train_test_split — разделение на train/test. Параметры: test_size, random_state, stratify для сохранения пропорций классов.\",\n",
        "    \"StandardScaler — стандартизация (z-score): (x - mean) / std. fit_transform(X_train), transform(X_test).\",\n",
        "    \"MinMaxScaler — нормализация в диапазон [0, 1]. Чувствителен к выбросам.\",\n",
        "    \"LabelEncoder — преобразование категориальных меток в числа 0, 1, 2, ...\",\n",
        "    \"OneHotEncoder — one-hot кодирование категориальных признаков. sparse_output=False для плотной матрицы.\",\n",
        "    \"cross_val_score — кросс-валидация. Параметры: estimator, X, y, cv (число фолдов). Возвращает массив скоров.\",\n",
        "    \"GridSearchCV — поиск лучших гиперпараметров по сетке. param_grid — словарь параметров.\",\n",
        "    \"Pipeline — объединение препроцессинга и модели. Pipeline([('scaler', StandardScaler()), ('clf', LogisticRegression())])\",\n",
        "    \n",
        "    # sklearn - кластеризация\n",
        "    \"KMeans — кластеризация методом k-средних. Параметры: n_clusters, random_state. fit_predict(X).\",\n",
        "    \"DBSCAN — плотностная кластеризация. Параметры: eps (радиус), min_samples. Находит кластеры произвольной формы.\",\n",
        "    \"AgglomerativeClustering — иерархическая кластеризация. Параметры: n_clusters, linkage (ward/complete/average).\",\n",
        "    \n",
        "    # sklearn - снижение размерности\n",
        "    \"PCA — метод главных компонент. Параметры: n_components. fit_transform(X) для снижения размерности.\",\n",
        "    \"TSNE — t-SNE для визуализации высокоразмерных данных в 2D/3D. Параметры: n_components, perplexity.\",\n",
        "    \n",
        "    # pandas\n",
        "    \"pandas DataFrame — основная структура данных. df.head(), df.info(), df.describe() для просмотра.\",\n",
        "    \"df.dropna() — удаление строк с пропусками. df.fillna(value) — заполнение пропусков.\",\n",
        "    \"df.groupby('column').agg({'col2': 'mean'}) — группировка и агрегация.\",\n",
        "    \"pd.read_csv('file.csv') — загрузка CSV. Параметры: sep, encoding, index_col.\",\n",
        "    \"df.merge(df2, on='key') — соединение таблиц. how='left'/'right'/'inner'/'outer'.\",\n",
        "    \"df.apply(func, axis=1) — применение функции к строкам. axis=0 для столбцов.\",\n",
        "    \"df.loc[condition] — фильтрация по условию. df.loc[df['col'] > 5]\",\n",
        "    \"df.value_counts() — подсчёт уникальных значений. df['col'].value_counts()\",\n",
        "    \"df.pivot_table() — сводная таблица. values, index, columns, aggfunc.\",\n",
        "    \n",
        "    # numpy\n",
        "    \"numpy array — эффективный многомерный массив. np.array([1, 2, 3]), np.zeros((3, 4)), np.ones((2, 2)).\",\n",
        "    \"np.random.seed(42) — фиксация генератора случайных чисел для воспроизводимости.\",\n",
        "    \"np.mean(), np.std(), np.sum() — агрегатные функции. axis=0 по столбцам, axis=1 по строкам.\",\n",
        "    \"np.reshape() — изменение формы массива. arr.reshape(-1, 1) для преобразования в столбец.\",\n",
        "    \"np.concatenate(), np.vstack(), np.hstack() — объединение массивов.\",\n",
        "    \"np.where(condition, x, y) — условный выбор элементов.\",\n",
        "    \n",
        "    # datasets\n",
        "    \"load_iris() — датасет ирисов Фишера. 150 примеров, 4 признака, 3 класса. from sklearn.datasets import load_iris\",\n",
        "    \"load_digits() — рукописные цифры 8x8. 1797 примеров, 64 признака, 10 классов.\",\n",
        "    \"fetch_california_housing() — цены на жильё в Калифорнии. Задача регрессии. 20640 примеров.\",\n",
        "    \"make_classification(), make_regression() — генерация синтетических данных для тестирования моделей.\",\n",
        "    \"load_breast_cancer() — диагностика рака груди. 569 примеров, 30 признаков, 2 класса.\",\n",
        "    \n",
        "    # matplotlib / визуализация\n",
        "    \"plt.figure(figsize=(10, 6)) — создание фигуры с размером.\",\n",
        "    \"plt.plot(x, y), plt.scatter(x, y), plt.bar(x, y), plt.hist(data) — типы графиков.\",\n",
        "    \"plt.xlabel(), plt.ylabel(), plt.title(), plt.legend() — подписи.\",\n",
        "    \"plt.savefig('plot.png', dpi=300) — сохранение графика.\",\n",
        "    \"plt.subplot(rows, cols, index) — создание сетки графиков.\",\n",
        "    \"seaborn.heatmap(data) — тепловая карта. annot=True для отображения значений.\",\n",
        "    \n",
        "    # FAISS\n",
        "    \"FAISS — библиотека Facebook для быстрого поиска ближайших соседей. IndexFlatL2 для точного поиска.\",\n",
        "    \"faiss.IndexFlatL2(dim) — создание индекса. index.add(vectors), index.search(query, k).\",\n",
        "    \"faiss.write_index(index, path) — сохранение индекса. faiss.read_index(path) — загрузка.\",\n",
        "    \n",
        "    # общие советы\n",
        "    \"Всегда фиксируйте random_state для воспроизводимости результатов.\",\n",
        "    \"Используйте stratify в train_test_split для сбалансированного разбиения при несбалансированных классах.\",\n",
        "    \"Масштабируйте признаки (StandardScaler/MinMaxScaler) перед обучением линейных моделей и SVM.\",\n",
        "    \"Для деревьев и ансамблей (RandomForest, GradientBoosting) масштабирование не требуется.\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "KKsDpb19HvFq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Добавлено 64 документов. Всего: 64\n"
          ]
        }
      ],
      "source": [
        "retriever = RetrieverAgent(docs)\n",
        "memory = MemoryAgent()\n",
        "executor = ColabExecutor()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "rntngyPiHvVY",
        "outputId": "d8961f69-1b1d-4d8b-fda6-034753892a9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.0\n",
            "Action: run_code\n",
            "Result: {'status': 'success', 'output': '{\\'np\\': <module \\'numpy\\' from \\'/Users/riter/anaconda3/envs/autorag/lib/python3.11/site-packages/numpy/__init__.py\\'>, \\'load_iris\\': <function load_iris at 0x30c5fe...\n",
            "\n",
            "Action: final_answer\n",
            "Result: Ниже — короткий исправленный пример с учётом замечаний (stratify, n_jobs, cross_val_score, classification_report):\n",
            "\n",
            "from sklearn.datasets import load_iris\n",
            "from sklearn.model_selection import train_tes...\n",
            "\n",
            "Ниже — короткий исправленный пример с учётом замечаний (stratify, n_jobs, cross_val_score, classification_report):\n",
            "\n",
            "from sklearn.datasets import load_iris\n",
            "from sklearn.model_selection import train_test_split, cross_val_score\n",
            "from sklearn.ensemble import RandomForestClassifier\n",
            "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
            "\n",
            "# Загрузка и разбиение (сохранение пропорций классов)\n",
            "iris = load_iris()\n",
            "X, y = iris.data, iris.target\n",
            "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
            "\n",
            "# Модель (фиксируем random_state и используем все ядра)\n",
            "clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
            "clf.fit(X_train, y_train)\n",
            "\n",
            "# Оценка на тесте\n",
            "y_pred = clf.predict(X_test)\n",
            "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
            "print(\"\\nClassification report:\\n\", classification_report(y_test, y_pred))\n",
            "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
            "\n",
            "# Кросс-валидация для более надёжной оценки\n",
            "cv_scores = cross_val_score(clf, X, y, cv=5, n_jobs=-1)\n",
            "print(\"\\nCross-val scores (5-fold):\", cv_scores)\n",
            "print(\"Mean CV accuracy:\", cv_scores.mean())\n",
            "\n",
            "Кратко:\n",
            "- В вашем предыдущем запуске accuracy на тесте получилось 1.0 — это нормально для iris, но лучше оценивать модель через кросс-валидацию.\n",
            "- np.random.seed(42) здесь необязателен, т.к. sklearn использует random_state; можно добавить, если вы явно используете numpy-рандом в коде.\n",
            "\n",
            "Если хотите, могу запустить этот код и показать конкретный вывод или вернуть только cross-val mean/построить отчёт визуально.\n"
          ]
        }
      ],
      "source": [
        "answer = jarvis_loop(\n",
        "    \"Обучи RandomForest на iris и выведи accuracy\",\n",
        "    retriever,\n",
        "    memory,\n",
        "    executor,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(answer)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
